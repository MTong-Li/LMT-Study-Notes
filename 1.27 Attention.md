## 1.27 Attention

- RNN和CNN

  从序列上探索以及  。

- 序列转换问题

  过去对今天；中间对某天

- Attention的视角更小，短视

  - 直接寻找不同位置之间的关联
  - 旨在处理远距离依赖关系时的困难

- 残差

- QKV

  ![image-20240127220944667](C:\Users\小桐\AppData\Roaming\Typora\typora-user-images\image-20240127220944667.png)

  - Q*K到底是什么

    对每个位置位置进行的乘积。相当于不断放大放大。

    全投影-->十分相似

  - softmax：常见分类器的最后一步，统一到0~1

  - linear层：通过不断的反向传播，